# Энграмма Шунъити Амари

> **Короткий портрет идей.** Амари предлагает смотреть на обучение как на **геометрию распределений**: модель — это многообразие вероятностных распределений с метрикой Фишера; «правильные» алгоритмы — это те, что уважают эту геометрию и инвариантны к переparameterизациям. Из этой оптики рождаются **естественный градиент**, **двойственные связи** (e/m), **проекции** и **дивергенции** как «энергии» задачи. 

---

## 1) Картина мира Амари: *геометрия как язык обучения*

* **Многообразие распределений** ( \mathcal{M} ) снабжено **метрикой Фишера** — естественным тензором, измеряющим «скорость» изменения правдоподобия; на нём определены **две сопряжённые аффинные связи** (exponential / mixture, e- и m‑связи). Это делает рассуждения **координатно‑свободными**. 
* **Теорема Ченцова** (и её расширения) объясняет, почему выбор именно метрики Фишера уникален среди монотонных метрик (с точностью до константы): это фундамент инвариантности, на котором строится natural gradient. 

---

## 2) Естественный (natural) градиент: *градиент, согласованный с геометрией*

* **Оптика**: самый крутой спуск не в евклидовой норме, а в норме метрики Фишера. Обновление
  [
  \theta_{t+1}=\theta_t-\eta,F(\theta_t)^{-1}\nabla_\theta L(\theta_t)
  ]
  инвариантно к перепараметризациям и устраняет «иллюзии координат» (когда скорость спуска — артефакт выбора параметров). Амари показал и **асимптотическую статистическую эффективность** таких обновлений. 
* **Взгляд «второго порядка»**: NGD — практически метод второго порядка, где метрика Фишера играет роль «хорошей» кривизны (часто совпадает с обобщённой Гаусса–Ньютоновской матрицей). Полезные следствия — доверительные регионы, регуляризация Тихонова и т. п. 

**Анти‑принцип:** не измеряй «строгость шага» в неправильной норме (евклидовой) — это даёт ложные выводы о скорости, затяжных плато и «плохих» координатах. 

---

## 3) Двойственность, проекции и Пифагор

* На dually‑flat многообразиях (напр., экспоненциальных семействах) работают **две сопряжённые геодезики** и **обобщённая теорема Пифагора** для KL/Bregman‑дивергенций — ключ к корректным аппроксимациям и итеративным схемам. 
* **EM как геометрия**: классический EM — это чередование e‑ и m‑проекций; «em»‑алгоритм — его информационно‑геометрическая формулировка. Это даёт ясный критерий, почему и куда сходятся такие процедуры. 

---

## 4) Дивергенции как «энергии» задачи

* Семейство **α‑дивергенций** (включая KL и Hellinger как частные случаи) — «регулятор» между e‑ и m‑миром; выбор α меняет компромисс между «покрыть поддержку» и «подогнать пики». Это семейство и связанные с ним проекционные теоремы — рабочие инструменты дизайна алгоритмов. 

---

## 5) Алгоритмы с отпечатком Амари

**(a) Natural gradient в нейросетях — от точного к приближённому.** Точный NGD требует инверсии FIM — дорого; поэтому используют **аппроксимации**: диагональные/блочные, или **K‑FAC** (Кронекер‑факторизация по слоям), которая сохраняет большую часть геометрии, но остаётся вычислимо‑доступной. 

**(b) Зеркальные связи: natural gradient ↔ mirror descent.** Для экспоненциальных семейств **mirror descent с Bregman‑потенциалом эквивалентен NGD на двойственном многообразии** — это объединяет «геометрию» и «первый порядок». 

**(c) RL:** **Natural Policy Gradient** масштабирует градиент политики инверсией Фишера; **TRPO** реализует близкую идею через доверительную область — практичный компромисс «геометрия ↔ стабильность». 

**(d) Слепое разделение источников / ICA:** natural gradient дал устойчивые и эквариантные обновления и в этих задачах. 

---

## 6) Как мыслить «по‑Амари»: чек‑лист решений

1. **Начни с дивергенции и инварианта.** Что естественная «длина шага» в твоей задаче? Если есть правдоподобие — бери метрику Фишера; иначе подбери Bregman‑потенциал, согласующийся с твоей аппроксимацией. 
2. **Выбери координаты, а не «прикладные ухищрения».** Для экспоненциальных семейств работай в **природных/ожидаемых координатах**; переходи между ними через Лежандрову двойственность. 
3. **Проекции важнее «хаков».** Переформулируй алгоритм как чередование e‑/m‑проекций: это повышает предсказуемость сходимости (пример — EM). 
4. **Предобусловливание = геометризация.** Любой «умный» предобусловиватель стоит смотреть как приближение (F^{-1}): K‑FAC, блочно‑диагональные, low‑rank. 
5. **Евклидовый градиент — лишь частный случай.** Если результат чувствителен к перепараметризациям — ты ещё не «попал» в геометрию задачи. 

---

## 7) Мини‑плейбук внедрения natural gradient

1. **Сформулируй цель** (L(\theta)) и явный/эмпирический FIM (F(\theta)) (для классификации с кросс‑энтропией это часто «эмпирический Фишер»). 
2. **Выбери аппроксимацию** (F):

   * малые модели — точный/Чолески;
   * средние — блочно‑диагональный;
   * глубокие сети — **K‑FAC** по слоям. 
3. **Сделай шаг NGD**: ( \Delta\theta = -\eta , \widehat{F}^{-1} g ).
4. **Стабилизируй** шаги: регуляризация ( \widehat{F}+\lambda I ), доверительный регион (TRPO‑подобно), затухание (\eta). 
5. **Проверь инвариантность**: перепараметризуй слой/нормировку — динамика должна быть почти той же (с учётом шага). 

---

## 8) «Если бы Амари решал задачу…»

* *«Какая здесь естественная геометрия?»* → выбери дивергенцию/метрику, а не просто «оптимизатор». 
* *«Какую проекцию я делаю на самом деле?»* → перепиши шаги как e‑/m‑проекции. 
* *«Можно ли перейти к двойственным координатам и упростить?»* → включи Лежандрову двойственность. 
* *«Есть ли недорогая аппроксимация (F^{-1})?»* → попробуй K‑FAC/блок‑Фишер. 

---

## 9) Глоссарий по‑Амари (в 7 строках)

* **Метрика Фишера** — риманова метрика на (\mathcal{M}), измеряющая локальную чувствительность распределения; единственная монотонная (по Ченцову). 
* **Естественный градиент** — направление наискорейшего убывания в метрике Фишера; инвариантное к перепараметризациям. 
* **e/m‑связи** — экспоненциальная и смесительная аффинные связи; при (\alpha=\pm 1) — крайние точки семейства **α‑связей**; (\alpha=0) — связь Леви‑Чивиты метрики Фишера. 
* **e/m‑проекции** — ортогональные (в информационном смысле) проекции по соответствующим связям. 
* **α‑дивергенция** — семейство дивергенций, объединяющее KL (прямой/обратный) и Hellinger; управляет балансом «поддержка ↔ пики». 
* **Dually flat** — многообразие с парой плоских (в соответствующих координатах) связей; у экспоненциальных семейств оно возникает естественно. 
* **Mirror descent** — первый порядок на Bregman‑геометрии; эквивалентен NGD на двойственном многообразии для экспоненциальных семейств. 

---

## 10) Быстрые шаблоны применения (по задачам)

**Глубокая классификация (CE‑loss).**
• (\widehat{F}) ≈ эмпирический Фишер; шаг NGD с K‑FAC; регуляризация (+\lambda I). Контроль — сравнить траектории под перепараметризацией слоёв. 

**Реинфорсмент‑ленинг (политики).**
• Натуральный градиент политики: ( \Delta\theta \propto F^{-1} g ); ограничение шага по KL (TRPO). 

**Латентные модели/скрытые переменные.**
• Формулируй E/M как e‑/m‑проекции; выбирай α‑дивергенцию, чтобы стабилизировать аппроксимации. 

**Сигналы и смешения (ICA/BSS).**
• Используй «относительный/естественный» градиент — эквариантность и стабильность к масштабам. 

---

## Референс‑карточка (на 30 секунд)

* **Ментальная модель:** «*Сначала геометрия (метрика/дивергенция), потом алгоритм*». 
* **Инструменты:** FIM, e/m‑связи, e/m‑проекции, α‑дивергенции, Пифагор/проекционные теоремы. 
* **Практика:** NGD (\approx) 2‑й порядок; в глубоком обучении — K‑FAC/блок‑Фишер; в RL — NPG/TRPO; в скрытых моделях — EM как чередование проекций. 
